{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 322 Final - Tehcnical Notebook\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "> ### Dataset Used:\n",
    ">> We used a very large (10GB) dataset provided by [Yelp](https://www.yelp.com/dataset) along with their corresponding [documentation](https://www.yelp.com/dataset/documentation/main) to identify the individual attributes, as there were a lot and many of which had unintuitive names such as 'compliment cute'. The documentation was extremely helpful.\n",
    "\n",
    "> ### Classification tasks implemented: \n",
    "\n",
    "> ### Findings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "> ### Dataset Information:\n",
    ">> Our dataset was initially too large to work with and contained a lot of uneccessary data. We started by identifying the most statistically significant attributes to test on by running a simple linear regression against 40 pair-wise attributes across all five JSON datasets from Yelp. Based on these we were able to identify the attributes from each instance which had the highest correlations. We ended up using ~15,500 business instances along with the following attributes for each, which included types ranging from integers to object arrays.\n",
    "\n",
    "\n",
    "> ### Relevant Summary Stats:\n",
    "\n",
    ">> Ultimately, the attributes we used, having been identified as being the most statistically significant from our regressions, include:\n",
    "\n",
    ">> 1. review count\n",
    "1. attributes (business)\n",
    "1. review length\n",
    "1. fans\n",
    "1. compliment plain\n",
    "1. friends\n",
    "1. useful\n",
    "\n",
    ">> We then ran a regression to test the correlation of individual attributes against each other. The strongest correlations included:\n",
    "\n",
    ">> 1. Useful (count) vs Compliment-Plain\n",
    "1. Attribute Count vs Review Length\n",
    "\n",
    ">> Moderate correlations:\n",
    "\n",
    ">> 1. Fans vs. Compliment Plain\n",
    "1. Useful vs Friends\n",
    "1. Useful vs Fans\n",
    "\n",
    "\n",
    "> ### Data Visualization\n",
    "\n",
    ">> We then applied discretization to generate frequency diagrams for each of the attributes to visualize a distribution of values by 'category' or bin values.\n",
    "\n",
    "\n",
    ">> *Review Count* - Represents the distribution of the number of reviews per business with count representing the number of instances with that review count value.\n",
    "\n",
    ">> <img src='./Images/Review-Count-Hist.PNG' width='500px'>\n",
    "\n",
    "\n",
    ">> *Review Length* - Represents the distribution of review lengths, with Count representing the number of business instances with that category (bin) of review length.\n",
    "\n",
    ">> <img src='./Images/Review-Length-Hist.PNG' width='500px'>\n",
    "\n",
    "\n",
    ">> *Attribute Counts* - Represents the distribution of the number of attributes (object in dataset) per business with Count representing the number of instances (businesses) with that number of attributes)\n",
    "\n",
    ">> <img src='./Images/Attribute-Counts.PNG' width='500px'>\n",
    "\n",
    "\n",
    ">> *Fans* - Represents the distribution of fans per business with Count representing the number of business instances with a particular category (bin)\n",
    "\n",
    ">> <img src='./Images/Fans.PNG' width='500px'>\n",
    "\n",
    "\n",
    ">> *Compliment (plain)* - Represents the distribution of the number of average compliments per business. \n",
    "\n",
    ">> <img src='./Images/Compliment-Plain.PNG' width='500px'>\n",
    "\n",
    "\n",
    ">> *Friends* - Represents the distribution of the number of friends for businesses.\n",
    "\n",
    ">> <img src='./Images/Friends.PNG' width='500px'>\n",
    "\n",
    "\n",
    ">> *Useful* - Represents the distribution of the number of useful votes on businesses, binned/categorized by Useful counts, with Count (y) representing number of business instances per bin/\n",
    "\n",
    ">> <img src='./Images/Useful.PNG' width='500px'>\n",
    "\n",
    "\n",
    ">> #### Strongest Correlations\n",
    "\n",
    ">> <img src='./Images/Regression1.PNG' width='500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Results\n",
    "\n",
    "> ### Classification Approach:\n",
    "    >> We used the EDA to make an informed decision on how to cut our dataset down to only use attributes that would produce the highest accuracy for our classification. After removing the columns of the attributes that were weakly connected to the \"useful\" upvote classifier, we decided to use Naive Bayes as our third classifier as it produced both of our best results on passed assignments.\n",
    "    \n",
    "> ### Techniques Used:\n",
    "    >>  - Naive Bayes\n",
    "    >>  - Decision Tree\n",
    "    >>  - Random Forest\n",
    "    \n",
    "> ### How classifiers were evaluated:\n",
    "    >> We ran each classifier through an accuracy and adjacency matrix test. In order to get the Random Forest to produce the best results possible, we tuned its parameters by running the classifier using various values of N, M, and F. After finding that an F value of 2, a low M value, and a high N value produced the best results, we ran those parameters multiple times to confirm the hypothesis. Once this was confirmed, we ran a final accuracy and adjacency matrix for Random Forest to be able to compare it to the likes of Naive Bayes and Decision Tree. \n",
    "    \n",
    "> ### Classifier Performance:\n",
    ">> #### Naive Bayes Vs. Decision Tree Accuracy\n",
    ">> <img src='./Images/NaiveBayesVsDT.PNG' width='500px'>\n",
    "    \n",
    ">> #### Naive Bayes Adjacency Matrix\n",
    ">> <img src='./Images/NaiveBayesAM.PNG' width='500px'>\n",
    "    \n",
    ">> #### Decision Tree Adjacency Matrix\n",
    ">> <img src='./Images/DTAM.PNG' width='500px'>\n",
    "    \n",
    ">> #### Random Forest M, N, and F Tuning\n",
    ">> <img src='./Images/MNFTuning.PNG' width='500px'>\n",
    "\n",
    ">> #### Hypothesis Testing for F=2\n",
    ">> <img src='./Images/HypothesisTesting.PNG' width='500px'>\n",
    "\n",
    ">> #### Final Random Forest Accuracy and Adjacency Matrix\n",
    ">> <img src='./Images/FinalRandomForest.PNG' width='500px'>\n",
    "\n",
    "> ### How porformance, comparison results, and best classifiers were evaluated:\n",
    ">> Ultimately, we found that accuracy along with precision were the two metrics that our classifiers would be ranked on. Because precision and recall are saying the same thing for our dataset, being that being incorrect on a user's binned \"useful\" value when it is something else is the same as our predicion being incorrect when it is actually a specific value. There is no contextual difference between a \"false positive\" and a \"false negative\", they are just incorrect classifications on a binned value. With that in mind, Naive Bayes is the classifier we have chosen to deploy for our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "> This project was very interesting because something that we thought would be very usefule and feasible to classify with the dataset (a business's star rating) was actually proven to be unconnected to the attributes in the set per our EDA. We ended up using the number of times a user has had another user be a \"fan\" of their review, the number of times their review has been marked \"compliment plain\", and the number of friends the user has as attributes to predict how many times the user has had a reivew upvoted, or in Yelp terms, marked as \"useful\". \n",
    "\n",
    "> We started with three classifiers: Naive Bayes, Decision Tree, and Random Forest, and after running those classifiers through tuning and accuracy testing, we decided that Naive Bayes was the most suited to make an accuracy classification on our dataset. \n",
    "\n",
    "> With a classifier chosen, we deployed that classifier to the following URL: ***INSERT URL***\n",
    "\n",
    "> If we were able to continue to work with this dataset, we think it would be useful to do an EDA across every single attribute in the Yelp JSON files. Do to the size of the files and the scope of this project, we were unable to do so. That being said, with more time, we may find other strongly connected attributes and develop classifiers and deploy them yet again.\n",
    "\n",
    "> Thank you, \n",
    "\n",
    ">Luke and Karsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

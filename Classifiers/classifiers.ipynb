{
 "cells": [
  {
   "source": [
    "# Classifiers Jupyter Notebook\n",
    "## Luke Mason & Karsen Hansen"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "# uncomment once you paste your mypytable.py into mysklearn package\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyNaiveBayesClassifier, MyDecisionTreeClassifier, MyRandomForestClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Fans', 'Compliment Plain', 'Friends', 'Useful']\n[12.0, 30.0, 37.0, 179.0]\n"
     ]
    }
   ],
   "source": [
    "mytable = MyPyTable()\n",
    "fName = os.path.join(\"input_data\", \"trimmed_data.csv\")\n",
    "dataset = mytable.load_from_file(fName)\n",
    "dataset.remove_rows_with_missing_values()\n",
    "rows_to_delete = ['Review Count', 'Attributes', 'Review Length']\n",
    "myutils.remove_rows_from_data(rows_to_delete, dataset)\n",
    "myutils.get_friend_count(2, dataset)\n",
    "print(dataset.column_names)\n",
    "print(dataset.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified k-fold Accuracy Check (Naive Bayes and Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Naive Bayes: accuracy = 0.8597194388777555 error = 0.1402805611222445\n",
      "Decision Tree: accuracy = 0.5404141616566466 error = 0.45958583834335337\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "#==================================\n",
    "#      Get stratified training    =\n",
    "#      and testing sets           =\n",
    "#==================================\n",
    "k = 10\n",
    "X, y = myutils.split_x_y_train(dataset.data)\n",
    "x_train, x_test, y_train, y_test = myevaluation.train_test_split(X, y, shuffle=True)\n",
    "\n",
    "# Get the training and testing folds\n",
    "train_folds, test_folds = myevaluation.stratified_kfold_cross_validation(X, y, k)\n",
    "\n",
    "# Get the traininga and testing sets from the folds\n",
    "x_train, y_train, x_test, y_test = myutils.get_values_from_folds(X, y, train_folds, test_folds)\n",
    "\n",
    "\n",
    "#==================================\n",
    "#      Naive Bayes Classifier     =\n",
    "#==================================\n",
    "myNB = MyNaiveBayesClassifier()\n",
    "myNB.fit(x_train, y_train)\n",
    "\n",
    "# Compare predicted with actual\n",
    "y_predict_nb = myNB.predict(x_test)\n",
    "count = 0\n",
    "for i in range(len(y_predict_nb)):\n",
    "    binned_predict = myutils.get_useful_bin(y_predict_nb[i])\n",
    "    binned_test = myutils.get_useful_bin(y_test[i])\n",
    "    if (binned_predict == binned_test):\n",
    "        count = count + 1;\n",
    "\n",
    "# Calculate accuracy and error\n",
    "accuracy = count / len(y_predict_nb)\n",
    "error = (len(y_predict_nb) - count) / len(y_predict_nb)\n",
    "\n",
    "print(\"Naive Bayes: accuracy =\", accuracy, \"error =\", error)\n",
    "\n",
    "#==================================\n",
    "#     Decision Tree Classifier    =\n",
    "#==================================\n",
    "myDT = MyDecisionTreeClassifier()\n",
    "myDT_x_train = copy.deepcopy(x_train)\n",
    "myDT_y_train = copy.deepcopy(y_train)\n",
    "myDT.fit(myDT_x_train, myDT_y_train)\n",
    "\n",
    "y_predict_dt = myDT.predict(x_test)\n",
    "count = 0\n",
    "for i in range(len(y_predict_dt)):\n",
    "    binned_predict = myutils.get_useful_bin(y_predict_dt[i])\n",
    "    binned_test = myutils.get_useful_bin(y_test[i])\n",
    "    if (binned_predict == binned_test):\n",
    "        count = count + 1;\n",
    "\n",
    "accuracy = count / len(y_predict_dt)\n",
    "error = (len(y_predict_dt) - count) / len(y_predict_dt)\n",
    "\n",
    "print(\"Decision Tree: accuracy =\", accuracy, \"error =\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices (Naive Bayes & Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nNaive Bayes\n========  ===  ===  ===  ===  ===  ===  ===  ===  ===  ====  =======  =================\n  Useful    1    2    3    4    5    6    7    8    9    10    Total    Recognition (%)\n========  ===  ===  ===  ===  ===  ===  ===  ===  ===  ====  =======  =================\n       1  167   21    9    4    1    0    0    0    0     0      202              82.67\n       2   63  130   14    5    3    0    0    0    0     0      215              60.47\n       3   23   18  120    8    4    0    0    0    0     0      173              69.36\n       4    8    7   12  108    3    0    0    0    0     0      138              78.26\n       5    0    2    3    2  251    0    0    0    0     0      258              97.29\n       6    0    0    0    0    0  183    0    0    0     0      183             100\n       7    0    0    0    0    0    0   81    0    0     0       81             100\n       8    0    0    0    0    0    0    0   64    0     0       64             100\n       9    0    0    0    0    0    0    0    0   49     0       49             100\n      10    0    0    0    0    0    0    0    0    0   134      134             100\n========  ===  ===  ===  ===  ===  ===  ===  ===  ===  ====  =======  =================\n"
     ]
    }
   ],
   "source": [
    "# Get the x values to perform the matrix over\n",
    "x = []\n",
    "for i in range(10):\n",
    "    x.append(i + 1)\n",
    "\n",
    "actual = []\n",
    "predicted = []\n",
    "for i in range(len(y_test)):\n",
    "    actual.append(myutils.get_useful_bin(y_test[i]))\n",
    "    predicted.append(myutils.get_useful_bin(y_predict_nb[i]))\n",
    "\n",
    "# Get the matrix from stratified\n",
    "matrix = myevaluation.confusion_matrix(actual, predicted, x)\n",
    "\n",
    "# Make the table header and calculate the statistics\n",
    "table_header = ['Useful', 1,2,3,4,5,6,7,8,9,10, 'Total', 'Recognition (%)']\n",
    "complete_matrix = myutils.calc_matrix_stats(matrix, False)\n",
    "\n",
    "print()\n",
    "print('Naive Bayes')\n",
    "myutils.print_tabulate(complete_matrix, table_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nDecision Tree\n========  ===  ===  ===  ===  ===  ===  ===  ===  ===  ====  =======  =================\n  Useful    1    2    3    4    5    6    7    8    9    10    Total    Recognition (%)\n========  ===  ===  ===  ===  ===  ===  ===  ===  ===  ====  =======  =================\n       1  157   26    3    5    5    3    0    2    1     0      202              77.72\n       2   74   89   16   13   11    9    2    1    0     0      215              41.4\n       3   50   37   61    9   11    5    0    0    0     0      173              35.26\n       4   39   18   15   52   10    4    0    0    0     0      138              37.68\n       5   30   47   28   20  111   12    6    1    2     1      258              43.02\n       6   13   20   15   10   15   99    7    2    1     1      183              54.1\n       7    7    9    5    1    3    2   50    1    3     0       81              61.73\n       8    2    5    3    4    8    5    1   34    1     1       64              53.12\n       9    1    2    2    1    4    0    2    0   37     0       49              75.51\n      10    0    1    2    0    1    0    3    4    4   119      134              88.81\n========  ===  ===  ===  ===  ===  ===  ===  ===  ===  ====  =======  =================\n"
     ]
    }
   ],
   "source": [
    "# Get the x values to perform the matrix over\n",
    "x = []\n",
    "for i in range(10):\n",
    "    x.append(i + 1)\n",
    "\n",
    "actual = []\n",
    "predicted = []\n",
    "for i in range(len(y_test)):\n",
    "    actual.append(myutils.get_useful_bin(y_test[i]))\n",
    "    predicted.append(myutils.get_useful_bin(y_predict_dt[i]))\n",
    "\n",
    "# Get the matrix from stratified\n",
    "matrix = myevaluation.confusion_matrix(actual, predicted, x)\n",
    "\n",
    "# Make the table header and calculate the statistics\n",
    "table_header = ['Useful', 1,2,3,4,5,6,7,8,9,10, 'Total', 'Recognition (%)']\n",
    "complete_matrix = myutils.calc_matrix_stats(matrix, False)\n",
    "\n",
    "print()\n",
    "print('Decision Tree')\n",
    "myutils.print_tabulate(complete_matrix, table_header)"
   ]
  },
  {
   "source": [
    "## Random Forest Parameter Tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Decision Tree: accuracy = 0.5390781563126252 error = 0.46092184368737477\n",
      "M=10, N=100\n",
      "0 -- accuracy = 0.8403473613894455 error = 0.15965263861055445\n",
      "1 -- accuracy = 0.8403473613894455 error = 0.15965263861055445\n",
      "2 -- accuracy = 0.8403473613894455 error = 0.15965263861055445\n",
      "3 -- accuracy = 0.8403473613894455 error = 0.15965263861055445\n",
      "4 -- accuracy = 0.8403473613894455 error = 0.15965263861055445\n",
      "M=10, N=500\n",
      "0 -- accuracy = 0.8403473613894455 error = 0.15965263861055445\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-c317fd7071f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mmyRF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mmyRF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremainder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0my_predict_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyRF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/Project/CPSC322FinalProject/Classifiers/mysklearn/myclassifiers.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, remainder, N, M)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruned_forest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_forest_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremainder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/Project/CPSC322FinalProject/Classifiers/mysklearn/myutils.py\u001b[0m in \u001b[0;36mrandom_forest_generation\u001b[0;34m(remainder_set, N, M)\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_x_y_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0mmyDT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m         \u001b[0mmyDT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m         \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyDT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/Project/CPSC322FinalProject/Classifiers/mysklearn/myclassifiers.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# initial call to tdidt current instances is the whole table (train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mavailable_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# python is pass object reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtdidt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavailable_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute_domains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/Project/CPSC322FinalProject/Classifiers/mysklearn/myutils.py\u001b[0m in \u001b[0;36mtdidt\u001b[0;34m(current_instances, available_attributes, attribute_domains, header)\u001b[0m\n\u001b[1;32m    866\u001b[0m     \"\"\"\n\u001b[1;32m    867\u001b[0m     \u001b[0;31m# Get the attribute to split on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     \u001b[0msplit_attribute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_instances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavailable_attributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m     \u001b[0mavailable_attributes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_attribute\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Clear the selected attribute from the available attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/Project/CPSC322FinalProject/Classifiers/mysklearn/myutils.py\u001b[0m in \u001b[0;36mbest_split\u001b[0;34m(training_set, available_attributes)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mgroup_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m             \u001b[0minstances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/Project/CPSC322FinalProject/Classifiers/mysklearn/myutils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mgroup_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m             \u001b[0minstances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "#==================================\n",
    "#      Get stratified training    =\n",
    "#      and testing sets           =\n",
    "#==================================\n",
    "# k = 10\n",
    "X, y = myutils.split_x_y_train(dataset.data)\n",
    "x_train, x_test, y_train, y_test = myevaluation.train_test_split(X, y, shuffle=True)\n",
    "\n",
    "# Get the training and testing folds\n",
    "train_folds, test_folds = myevaluation.stratified_kfold_cross_validation(X, y, k)\n",
    "\n",
    "# Get the traininga and testing sets from the folds\n",
    "x_train, y_train, x_test, y_test = myutils.get_values_from_folds(X, y, train_folds, test_folds)\n",
    "\n",
    "#==================================\n",
    "#     Decision Tree Classifier    =\n",
    "#==================================\n",
    "myDT = MyDecisionTreeClassifier()\n",
    "myDT_x_train = copy.deepcopy(x_train)\n",
    "myDT_y_train = copy.deepcopy(y_train)\n",
    "myDT.fit(myDT_x_train, myDT_y_train)\n",
    "\n",
    "y_predict_dt = myDT.predict(x_test)\n",
    "count = 0\n",
    "for i in range(len(y_predict_dt)):\n",
    "    binned_predict = myutils.get_useful_bin(y_predict_dt[i])\n",
    "    binned_test = myutils.get_useful_bin(y_test[i])\n",
    "    if (binned_predict == binned_test):\n",
    "        count = count + 1;\n",
    "\n",
    "accuracy = count / len(y_predict_dt)\n",
    "error = (len(y_predict_dt) - count) / len(y_predict_dt)\n",
    "\n",
    "print(\"Decision Tree: accuracy =\", accuracy, \"error =\", error)\n",
    "\n",
    "\n",
    "#==================================\n",
    "#     Random Forest Classifier    =\n",
    "#==================================\n",
    "\n",
    "#==================================\n",
    "#          M: 10, N: 100          =\n",
    "#==================================\n",
    "print(\"M=10, N=100\")\n",
    "for i in range(5):\n",
    "    X, y = myutils.split_x_y_train(dataset.data)\n",
    "    x_train, x_test, y_train, y_test = myevaluation.train_test_split(X, y, shuffle=True)\n",
    "\n",
    "    # Get the training and testing folds\n",
    "    train_folds, test_folds = myevaluation.stratified_kfold_cross_validation(X, y, k)\n",
    "\n",
    "    # Get the traininga and testing sets from the folds\n",
    "    x_train, y_train, x_test, y_test = myutils.get_values_from_folds(X, y, train_folds, test_folds)\n",
    "\n",
    "    remainder = []\n",
    "\n",
    "    for j in range(len(x_train)):\n",
    "        row = x_train[j]\n",
    "        row.append(y_train[j])\n",
    "        remainder.append(row)\n",
    "\n",
    "    myRF = MyRandomForestClassifier()\n",
    "    myRF.fit(remainder, 10, 100)\n",
    "\n",
    "    y_predict_rf = myRF.predict(x_test)\n",
    "    count = 0\n",
    "    for l in range(len(y_predict_rf)):\n",
    "        binned_predict = myutils.get_useful_bin(y_predict_rf[l])\n",
    "        binned_test = myutils.get_useful_bin(y_test[l])\n",
    "        if (binned_predict == binned_test):\n",
    "            count = count + 1;\n",
    "\n",
    "    accuracy = count / len(y_predict_rf)\n",
    "    error = (len(y_predict_rf) - count) / len(y_predict_rf)\n",
    "    print(i, \"-- accuracy =\", accuracy, \"error =\", error)\n",
    "\n",
    "#==================================\n",
    "#          M: 10, N: 500          =\n",
    "#==================================\n",
    "print(\"M=10, N=500\")\n",
    "for i in range(5):\n",
    "    X, y = myutils.split_x_y_train(dataset.data)\n",
    "    x_train, x_test, y_train, y_test = myevaluation.train_test_split(X, y, shuffle=True)\n",
    "\n",
    "    # Get the training and testing folds\n",
    "    train_folds, test_folds = myevaluation.stratified_kfold_cross_validation(X, y, k)\n",
    "\n",
    "    # Get the traininga and testing sets from the folds\n",
    "    x_train, y_train, x_test, y_test = myutils.get_values_from_folds(X, y, train_folds, test_folds)\n",
    "    \n",
    "    remainder = []\n",
    "\n",
    "    for j in range(len(x_train)):\n",
    "        row = x_train[j]\n",
    "        row.append(y_train[j])\n",
    "        remainder.append(row)\n",
    "\n",
    "    myRF = MyRandomForestClassifier()\n",
    "    myRF.fit(remainder, 10, 500)\n",
    "\n",
    "    y_predict_rf = myRF.predict(x_test)\n",
    "    count = 0\n",
    "    for l in range(len(y_predict_rf)):\n",
    "        binned_predict = myutils.get_useful_bin(y_predict_rf[l])\n",
    "        binned_test = myutils.get_useful_bin(y_test[l])\n",
    "        if (binned_predict == binned_test):\n",
    "            count = count + 1;\n",
    "\n",
    "    accuracy = count / len(y_predict_rf)\n",
    "    error = (len(y_predict_rf) - count) / len(y_predict_rf)\n",
    "    print(i, \"-- accuracy =\", accuracy, \"error =\", error)\n",
    "\n",
    "#==================================\n",
    "#          M: 100, N: 500         =\n",
    "#==================================\n",
    "print(\"M=100, N=500\")\n",
    "for i in range(5):\n",
    "    X, y = myutils.split_x_y_train(dataset.data)\n",
    "    x_train, x_test, y_train, y_test = myevaluation.train_test_split(X, y, shuffle=True)\n",
    "\n",
    "    # Get the training and testing folds\n",
    "    train_folds, test_folds = myevaluation.stratified_kfold_cross_validation(X, y, k)\n",
    "\n",
    "    # Get the traininga and testing sets from the folds\n",
    "    x_train, y_train, x_test, y_test = myutils.get_values_from_folds(X, y, train_folds, test_folds)\n",
    "    \n",
    "    remainder = []\n",
    "\n",
    "    for j in range(len(x_train)):\n",
    "        row = x_train[j]\n",
    "        row.append(y_train[j])\n",
    "        remainder.append(row)\n",
    "\n",
    "    myRF = MyRandomForestClassifier()\n",
    "    myRF.fit(remainder, 100, 500)\n",
    "\n",
    "    y_predict_rf = myRF.predict(x_test)\n",
    "    count = 0\n",
    "    for l in range(len(y_predict_rf)):\n",
    "        binned_predict = myutils.get_useful_bin(y_predict_rf[l])\n",
    "        binned_test = myutils.get_useful_bin(y_test[l])\n",
    "        if (binned_predict == binned_test):\n",
    "            count = count + 1;\n",
    "\n",
    "    accuracy = count / len(y_predict_rf)\n",
    "    error = (len(y_predict_rf) - count) / len(y_predict_rf)\n",
    "    print(i, \"-- accuracy =\", accuracy, \"error =\", error)\n",
    "\n",
    "#==================================\n",
    "#          M: 25, N: 50           =\n",
    "#==================================\n",
    "print(\"M=25, N=50\")\n",
    "for i in range(5):\n",
    "    X, y = myutils.split_x_y_train(dataset.data)\n",
    "    x_train, x_test, y_train, y_test = myevaluation.train_test_split(X, y, shuffle=True)\n",
    "\n",
    "    # Get the training and testing folds\n",
    "    train_folds, test_folds = myevaluation.stratified_kfold_cross_validation(X, y, k)\n",
    "\n",
    "    # Get the traininga and testing sets from the folds\n",
    "    x_train, y_train, x_test, y_test = myutils.get_values_from_folds(X, y, train_folds, test_folds)\n",
    "    \n",
    "    remainder = []\n",
    "\n",
    "    for j in range(len(x_train)):\n",
    "        row = x_train[j]\n",
    "        row.append(y_train[j])\n",
    "        remainder.append(row)\n",
    "\n",
    "    for p in range(5):\n",
    "        print(remainder[p])\n",
    "\n",
    "    myRF = MyRandomForestClassifier()\n",
    "    myRF.fit(remainder, 25, 50)\n",
    "\n",
    "    y_predict_rf = myRF.predict(x_test)\n",
    "    count = 0\n",
    "    for l in range(len(y_predict_rf)):\n",
    "        binned_predict = myutils.get_useful_bin(y_predict_rf[l])\n",
    "        binned_test = myutils.get_useful_bin(y_test[l])\n",
    "        if (binned_predict == binned_test):\n",
    "            count = count + 1;\n",
    "\n",
    "    accuracy = count / len(y_predict_rf)\n",
    "    error = (len(y_predict_rf) - count) / len(y_predict_rf)\n",
    "    print(i, \"-- accuracy =\", accuracy, \"error =\", error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'y_predict_rf' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-32bfacd562a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mactual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_useful_bin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_useful_bin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict_rf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Get the matrix from stratified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_predict_rf' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the x values to perform the matrix over\n",
    "x = []\n",
    "for i in range(10):\n",
    "    x.append(i + 1)\n",
    "\n",
    "actual = []\n",
    "predicted = []\n",
    "for i in range(len(y_test)):\n",
    "    actual.append(myutils.get_useful_bin(y_test[i]))\n",
    "    predicted.append(myutils.get_useful_bin(y_predict_rf[i]))\n",
    "\n",
    "# Get the matrix from stratified\n",
    "matrix = myevaluation.confusion_matrix(actual, predicted, x)\n",
    "\n",
    "# Make the table header and calculate the statistics\n",
    "table_header = ['Useful', 1,2,3,4,5,6,7,8,9,10, 'Total', 'Recognition (%)']\n",
    "complete_matrix = myutils.calc_matrix_stats(matrix, False)\n",
    "\n",
    "print()\n",
    "print('Random Forest')\n",
    "myutils.print_tabulate(complete_matrix, table_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}